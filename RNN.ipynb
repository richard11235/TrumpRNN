{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, MaxPooling2D, ReLU, GRU, Softmax, Embedding, Layer\n",
    "#from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D, LSTM\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(128,128,batch_input_shape=[1,1]))\n",
    "    model.add(GRU(256,\n",
    "                return_sequences=True,\n",
    "                stateful=True,\n",
    "                recurrent_initializer='glorot_uniform'))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Softmax())\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def load_data(txtfile='data/content.txt'):\n",
    "    f = open(txtfile,'r',encoding='utf-8')\n",
    "    lines = f.read().lower()\n",
    "    f.close()\n",
    "    lines = [list(map(ord,line)) + [ord('\\n')] for line in lines.split('\\n') if is_ascii(line)]\n",
    "    return lines\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "def train(model,sentence_list,batch=4,epochs=1000,interval=1000):\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        idxs = np.random.randint(0, len(sentence_list), batch)\n",
    "        for idx in idxs:\n",
    "            model.reset_states()\n",
    "            line = sentence_list[idx]\n",
    "            for char in range(len(line)-1):\n",
    "                dic = model.train_on_batch(np.array(line[char]).reshape(1,1),num2arr(line[char+1]), reset_metrics=True, return_dict=True)\n",
    "                loss = loss + dic['loss']\n",
    "        print(\"Loss: \",loss/batch)\n",
    "        RNN.reset_states()\n",
    "        \n",
    "def num2arr(num):\n",
    "    arr = np.zeros(128)\n",
    "    arr[num] = 1\n",
    "    return arr.reshape((1,1,128))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence(model,prompt=None):\n",
    "    model.reset_states()\n",
    "    if prompt == None:\n",
    "      start = np.random.randint(97,123)\n",
    "      char = chr(start)\n",
    "    else:\n",
    "      char = prompt\n",
    "      model.predict(np.array(list(char)).reshape((1,len(prompt),1)))\n",
    "    string = char\n",
    "    string_ = char\n",
    "    no = 0\n",
    "    while char != '\\n' and no < 140:\n",
    "        p = model.predict(np.array([start]).reshape((1,1)))\n",
    "        char = np.random.choice(range(128),p=p.ravel())\n",
    "        string = string + chr(char)\n",
    "        string_ = string_ + chr(np.argmax(p.ravel()))\n",
    "        \n",
    "        no = no + 1\n",
    "    return string, string_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (1, 1, 128)               16384     \n_________________________________________________________________\ngru_1 (GRU)                  (1, 1, 256)               296448    \n_________________________________________________________________\ndense_1 (Dense)              (1, 1, 128)               32896     \n_________________________________________________________________\nsoftmax_1 (Softmax)          (1, 1, 128)               0         \n=================================================================\nTotal params: 345,728\nTrainable params: 345,728\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "RNN = build_model()\n",
    "RNN.compile(loss='categorical_crossentropy',\n",
    "            optimizer=Adam(0.0002, .5),\n",
    "            metrics=['accuracy'])\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch:  1\nEpoch:  2\nEpoch:  3\nEpoch:  4\nEpoch:  5\nEpoch:  6\nEpoch:  7\nEpoch:  8\nEpoch:  9\nEpoch:  10\nEpoch:  11\nEpoch:  12\nEpoch:  13\nEpoch:  14\nEpoch:  15\nEpoch:  16\nEpoch:  17\nEpoch:  18\nEpoch:  19\nEpoch:  20\nEpoch:  21\nEpoch:  22\nEpoch:  23\nEpoch:  24\nEpoch:  25\nEpoch:  26\nEpoch:  27\nEpoch:  28\nEpoch:  29\nEpoch:  30\nEpoch:  31\nEpoch:  32\nEpoch:  33\nEpoch:  34\nEpoch:  35\nEpoch:  36\nEpoch:  37\nEpoch:  38\nEpoch:  39\nEpoch:  40\nEpoch:  41\nEpoch:  42\nEpoch:  43\nEpoch:  44\nEpoch:  45\nEpoch:  46\nEpoch:  47\nEpoch:  48\nEpoch:  49\nEpoch:  50\nEpoch:  51\nEpoch:  52\nEpoch:  53\nEpoch:  54\nEpoch:  55\nEpoch:  56\nEpoch:  57\nEpoch:  58\nEpoch:  59\nEpoch:  60\nEpoch:  61\nEpoch:  62\nEpoch:  63\nEpoch:  64\nEpoch:  65\nEpoch:  66\nEpoch:  67\nEpoch:  68\nEpoch:  69\nEpoch:  70\nEpoch:  71\nEpoch:  72\nEpoch:  73\nEpoch:  74\nEpoch:  75\nEpoch:  76\nEpoch:  77\nEpoch:  78\nEpoch:  79\nEpoch:  80\nEpoch:  81\nEpoch:  82\nEpoch:  83\nEpoch:  84\nEpoch:  85\nEpoch:  86\nEpoch:  87\nEpoch:  88\nEpoch:  89\nEpoch:  90\nEpoch:  91\nEpoch:  92\nEpoch:  93\nEpoch:  94\nEpoch:  95\nEpoch:  96\nEpoch:  97\nEpoch:  98\nEpoch:  99\nEpoch:  100\nEpoch:  101\nEpoch:  102\nEpoch:  103\nEpoch:  104\nEpoch:  105\nEpoch:  106\nEpoch:  107\nEpoch:  108\nEpoch:  109\nEpoch:  110\nEpoch:  111\nEpoch:  112\nEpoch:  113\nEpoch:  114\nEpoch:  115\nEpoch:  116\nEpoch:  117\nEpoch:  118\nEpoch:  119\nEpoch:  120\nEpoch:  121\nEpoch:  122\nEpoch:  123\nEpoch:  124\nEpoch:  125\nEpoch:  126\nEpoch:  127\nEpoch:  128\nEpoch:  129\nEpoch:  130\nEpoch:  131\nEpoch:  132\nEpoch:  133\nEpoch:  134\nEpoch:  135\nEpoch:  136\nEpoch:  137\nEpoch:  138\nEpoch:  139\nEpoch:  140\nEpoch:  141\nEpoch:  142\nEpoch:  143\nEpoch:  144\nEpoch:  145\nEpoch:  146\nEpoch:  147\nEpoch:  148\nEpoch:  149\nEpoch:  150\nEpoch:  151\nEpoch:  152\nEpoch:  153\nEpoch:  154\nEpoch:  155\nEpoch:  156\nEpoch:  157\nEpoch:  158\nEpoch:  159\nEpoch:  160\nEpoch:  161\nEpoch:  162\nEpoch:  163\nEpoch:  164\nEpoch:  165\nEpoch:  166\nEpoch:  167\nEpoch:  168\nEpoch:  169\nEpoch:  170\nEpoch:  171\nEpoch:  172\nEpoch:  173\nEpoch:  174\nEpoch:  175\nEpoch:  176\nEpoch:  177\nEpoch:  178\nEpoch:  179\nEpoch:  180\nEpoch:  181\nEpoch:  182\nEpoch:  183\nEpoch:  184\nEpoch:  185\nEpoch:  186\nEpoch:  187\nEpoch:  188\nEpoch:  189\nEpoch:  190\nEpoch:  191\nEpoch:  192\nEpoch:  193\nEpoch:  194\nEpoch:  195\nEpoch:  196\nEpoch:  197\nEpoch:  198\nEpoch:  199\nEpoch:  200\nEpoch:  201\nEpoch:  202\nEpoch:  203\nEpoch:  204\nEpoch:  205\nEpoch:  206\nEpoch:  207\nEpoch:  208\nEpoch:  209\nEpoch:  210\nEpoch:  211\nEpoch:  212\nEpoch:  213\nEpoch:  214\nEpoch:  215\nEpoch:  216\nEpoch:  217\nEpoch:  218\nEpoch:  219\nEpoch:  220\nEpoch:  221\nEpoch:  222\nEpoch:  223\nEpoch:  224\nEpoch:  225\nEpoch:  226\nEpoch:  227\nEpoch:  228\nEpoch:  229\nEpoch:  230\nEpoch:  231\nEpoch:  232\nEpoch:  233\nEpoch:  234\nEpoch:  235\nEpoch:  236\nEpoch:  237\nEpoch:  238\nEpoch:  239\nEpoch:  240\nEpoch:  241\nEpoch:  242\nEpoch:  243\nEpoch:  244\nEpoch:  245\nEpoch:  246\nEpoch:  247\nEpoch:  248\nEpoch:  249\nEpoch:  250\nEpoch:  251\nEpoch:  252\nEpoch:  253\nEpoch:  254\nEpoch:  255\nEpoch:  256\nEpoch:  257\nEpoch:  258\nEpoch:  259\nEpoch:  260\nEpoch:  261\nEpoch:  262\nEpoch:  263\nEpoch:  264\nEpoch:  265\nEpoch:  266\nEpoch:  267\nEpoch:  268\nEpoch:  269\nEpoch:  270\nEpoch:  271\nEpoch:  272\nEpoch:  273\nEpoch:  274\nEpoch:  275\nEpoch:  276\nEpoch:  277\nEpoch:  278\nEpoch:  279\nEpoch:  280\nEpoch:  281\nEpoch:  282\nEpoch:  283\nEpoch:  284\nEpoch:  285\nEpoch:  286\nEpoch:  287\nEpoch:  288\nEpoch:  289\nEpoch:  290\nEpoch:  291\nEpoch:  292\nEpoch:  293\nEpoch:  294\nEpoch:  295\nEpoch:  296\nEpoch:  297\nEpoch:  298\nEpoch:  299\nEpoch:  300\nEpoch:  301\nEpoch:  302\nEpoch:  303\nEpoch:  304\nEpoch:  305\nEpoch:  306\nEpoch:  307\nEpoch:  308\nEpoch:  309\nEpoch:  310\nEpoch:  311\nEpoch:  312\nEpoch:  313\nEpoch:  314\nEpoch:  315\nEpoch:  316\nEpoch:  317\nEpoch:  318\nEpoch:  319\nEpoch:  320\nEpoch:  321\nEpoch:  322\nEpoch:  323\nEpoch:  324\nEpoch:  325\nEpoch:  326\nEpoch:  327\nEpoch:  328\nEpoch:  329\nEpoch:  330\nEpoch:  331\nEpoch:  332\nEpoch:  333\nEpoch:  334\nEpoch:  335\nEpoch:  336\nEpoch:  337\nEpoch:  338\nEpoch:  339\nEpoch:  340\nEpoch:  341\nEpoch:  342\nEpoch:  343\nEpoch:  344\nEpoch:  345\nEpoch:  346\nEpoch:  347\nEpoch:  348\nEpoch:  349\nEpoch:  350\nEpoch:  351\nEpoch:  352\nEpoch:  353\nEpoch:  354\nEpoch:  355\nEpoch:  356\nEpoch:  357\nEpoch:  358\nEpoch:  359\nEpoch:  360\nEpoch:  361\nEpoch:  362\nEpoch:  363\nEpoch:  364\nEpoch:  365\nEpoch:  366\nEpoch:  367\nEpoch:  368\nEpoch:  369\nEpoch:  370\nEpoch:  371\nEpoch:  372\nEpoch:  373\nEpoch:  374\nEpoch:  375\nEpoch:  376\nEpoch:  377\nEpoch:  378\nEpoch:  379\nEpoch:  380\nEpoch:  381\nEpoch:  382\nEpoch:  383\nEpoch:  384\nEpoch:  385\nEpoch:  386\nEpoch:  387\nEpoch:  388\nEpoch:  389\nEpoch:  390\nEpoch:  391\nEpoch:  392\nEpoch:  393\nEpoch:  394\nEpoch:  395\nEpoch:  396\nEpoch:  397\nEpoch:  398\nEpoch:  399\nEpoch:  400\nEpoch:  401\nEpoch:  402\nEpoch:  403\nEpoch:  404\nEpoch:  405\nEpoch:  406\nEpoch:  407\nEpoch:  408\nEpoch:  409\nEpoch:  410\nEpoch:  411\nEpoch:  412\nEpoch:  413\nEpoch:  414\nEpoch:  415\nEpoch:  416\nEpoch:  417\nEpoch:  418\nEpoch:  419\nEpoch:  420\nEpoch:  421\nEpoch:  422\nEpoch:  423\nEpoch:  424\nEpoch:  425\nEpoch:  426\nEpoch:  427\nEpoch:  428\nEpoch:  429\nEpoch:  430\nEpoch:  431\nEpoch:  432\nEpoch:  433\nEpoch:  434\nEpoch:  435\nEpoch:  436\nEpoch:  437\nEpoch:  438\nEpoch:  439\nEpoch:  440\nEpoch:  441\nEpoch:  442\nEpoch:  443\nEpoch:  444\nEpoch:  445\nEpoch:  446\nEpoch:  447\nEpoch:  448\nEpoch:  449\nEpoch:  450\nEpoch:  451\nEpoch:  452\nEpoch:  453\nEpoch:  454\nEpoch:  455\nEpoch:  456\nEpoch:  457\nEpoch:  458\nEpoch:  459\nEpoch:  460\nEpoch:  461\nEpoch:  462\nEpoch:  463\nEpoch:  464\nEpoch:  465\nEpoch:  466\nEpoch:  467\nEpoch:  468\nEpoch:  469\nEpoch:  470\nEpoch:  471\nEpoch:  472\nEpoch:  473\nEpoch:  474\nEpoch:  475\nEpoch:  476\nEpoch:  477\nEpoch:  478\nEpoch:  479\nEpoch:  480\nEpoch:  481\nEpoch:  482\nEpoch:  483\nEpoch:  484\nEpoch:  485\nEpoch:  486\nEpoch:  487\nEpoch:  488\nEpoch:  489\nEpoch:  490\nEpoch:  491\nEpoch:  492\nEpoch:  493\nEpoch:  494\nEpoch:  495\nEpoch:  496\nEpoch:  497\nEpoch:  498\nEpoch:  499\nEpoch:  500\nEpoch:  501\nEpoch:  502\nEpoch:  503\nEpoch:  504\nEpoch:  505\nEpoch:  506\nEpoch:  507\nEpoch:  508\nEpoch:  509\nEpoch:  510\nEpoch:  511\nEpoch:  512\nEpoch:  513\nEpoch:  514\nEpoch:  515\nEpoch:  516\nEpoch:  517\nEpoch:  518\nEpoch:  519\nEpoch:  520\nEpoch:  521\nEpoch:  522\nEpoch:  523\nEpoch:  524\nEpoch:  525\nEpoch:  526\nEpoch:  527\nEpoch:  528\nEpoch:  529\nEpoch:  530\nEpoch:  531\nEpoch:  532\nEpoch:  533\nEpoch:  534\nEpoch:  535\nEpoch:  536\nEpoch:  537\nEpoch:  538\nEpoch:  539\nEpoch:  540\nEpoch:  541\nEpoch:  542\nEpoch:  543\nEpoch:  544\nEpoch:  545\nEpoch:  546\nEpoch:  547\nEpoch:  548\nEpoch:  549\nEpoch:  550\nEpoch:  551\nEpoch:  552\nEpoch:  553\nEpoch:  554\nEpoch:  555\nEpoch:  556\nEpoch:  557\nEpoch:  558\nEpoch:  559\nEpoch:  560\nEpoch:  561\nEpoch:  562\nEpoch:  563\nEpoch:  564\nEpoch:  565\nEpoch:  566\nEpoch:  567\nEpoch:  568\nEpoch:  569\nEpoch:  570\nEpoch:  571\nEpoch:  572\nEpoch:  573\nEpoch:  574\nEpoch:  575\nEpoch:  576\nEpoch:  577\nEpoch:  578\nEpoch:  579\nEpoch:  580\nEpoch:  581\nEpoch:  582\nEpoch:  583\nEpoch:  584\nEpoch:  585\nEpoch:  586\nEpoch:  587\nEpoch:  588\nEpoch:  589\nEpoch:  590\nEpoch:  591\nEpoch:  592\nEpoch:  593\nEpoch:  594\nEpoch:  595\nEpoch:  596\nEpoch:  597\nEpoch:  598\nEpoch:  599\nEpoch:  600\nEpoch:  601\nEpoch:  602\nEpoch:  603\nEpoch:  604\nEpoch:  605\nEpoch:  606\nEpoch:  607\nEpoch:  608\nEpoch:  609\nEpoch:  610\nEpoch:  611\nEpoch:  612\nEpoch:  613\nEpoch:  614\nEpoch:  615\nEpoch:  616\nEpoch:  617\nEpoch:  618\nEpoch:  619\nEpoch:  620\nEpoch:  621\nEpoch:  622\nEpoch:  623\nEpoch:  624\nEpoch:  625\nEpoch:  626\nEpoch:  627\nEpoch:  628\nEpoch:  629\nEpoch:  630\nEpoch:  631\nEpoch:  632\nEpoch:  633\nEpoch:  634\nEpoch:  635\nEpoch:  636\nEpoch:  637\nEpoch:  638\nEpoch:  639\nEpoch:  640\nEpoch:  641\nEpoch:  642\nEpoch:  643\nEpoch:  644\nEpoch:  645\nEpoch:  646\nEpoch:  647\nEpoch:  648\nEpoch:  649\nEpoch:  650\nEpoch:  651\nEpoch:  652\nEpoch:  653\nEpoch:  654\nEpoch:  655\nEpoch:  656\nEpoch:  657\nEpoch:  658\nEpoch:  659\nEpoch:  660\nEpoch:  661\nEpoch:  662\nEpoch:  663\nEpoch:  664\nEpoch:  665\nEpoch:  666\nEpoch:  667\nEpoch:  668\nEpoch:  669\nEpoch:  670\nEpoch:  671\nEpoch:  672\nEpoch:  673\nEpoch:  674\nEpoch:  675\nEpoch:  676\nEpoch:  677\nEpoch:  678\nEpoch:  679\nEpoch:  680\nEpoch:  681\nEpoch:  682\nEpoch:  683\nEpoch:  684\nEpoch:  685\nEpoch:  686\nEpoch:  687\nEpoch:  688\nEpoch:  689\nEpoch:  690\nEpoch:  691\nEpoch:  692\nEpoch:  693\nEpoch:  694\nEpoch:  695\nEpoch:  696\nEpoch:  697\nEpoch:  698\nEpoch:  699\nEpoch:  700\nEpoch:  701\nEpoch:  702\nEpoch:  703\nEpoch:  704\nEpoch:  705\nEpoch:  706\nEpoch:  707\nEpoch:  708\nEpoch:  709\nEpoch:  710\nEpoch:  711\nEpoch:  712\nEpoch:  713\nEpoch:  714\nEpoch:  715\nEpoch:  716\nEpoch:  717\nEpoch:  718\nEpoch:  719\nEpoch:  720\nEpoch:  721\nEpoch:  722\nEpoch:  723\nEpoch:  724\nEpoch:  725\nEpoch:  726\nEpoch:  727\nEpoch:  728\nEpoch:  729\nEpoch:  730\nEpoch:  731\nEpoch:  732\nEpoch:  733\nEpoch:  734\nEpoch:  735\nEpoch:  736\nEpoch:  737\nEpoch:  738\nEpoch:  739\nEpoch:  740\nEpoch:  741\nEpoch:  742\nEpoch:  743\nEpoch:  744\nEpoch:  745\nEpoch:  746\nEpoch:  747\nEpoch:  748\nEpoch:  749\nEpoch:  750\nEpoch:  751\nEpoch:  752\nEpoch:  753\nEpoch:  754\nEpoch:  755\nEpoch:  756\nEpoch:  757\nEpoch:  758\nEpoch:  759\nEpoch:  760\nEpoch:  761\nEpoch:  762\nEpoch:  763\nEpoch:  764\nEpoch:  765\nEpoch:  766\nEpoch:  767\nEpoch:  768\nEpoch:  769\nEpoch:  770\nEpoch:  771\nEpoch:  772\nEpoch:  773\nEpoch:  774\nEpoch:  775\nEpoch:  776\nEpoch:  777\nEpoch:  778\nEpoch:  779\nEpoch:  780\nEpoch:  781\nEpoch:  782\nEpoch:  783\nEpoch:  784\nEpoch:  785\nEpoch:  786\nEpoch:  787\nEpoch:  788\nEpoch:  789\nEpoch:  790\nEpoch:  791\nEpoch:  792\nEpoch:  793\nEpoch:  794\nEpoch:  795\nEpoch:  796\nEpoch:  797\nEpoch:  798\nEpoch:  799\nEpoch:  800\nEpoch:  801\nEpoch:  802\nEpoch:  803\nEpoch:  804\nEpoch:  805\nEpoch:  806\nEpoch:  807\nEpoch:  808\nEpoch:  809\nEpoch:  810\nEpoch:  811\nEpoch:  812\nEpoch:  813\nEpoch:  814\nEpoch:  815\nEpoch:  816\nEpoch:  817\nEpoch:  818\nEpoch:  819\nEpoch:  820\nEpoch:  821\nEpoch:  822\nEpoch:  823\nEpoch:  824\nEpoch:  825\nEpoch:  826\nEpoch:  827\nEpoch:  828\nEpoch:  829\nEpoch:  830\nEpoch:  831\nEpoch:  832\nEpoch:  833\nEpoch:  834\nEpoch:  835\nEpoch:  836\nEpoch:  837\nEpoch:  838\nEpoch:  839\nEpoch:  840\nEpoch:  841\nEpoch:  842\nEpoch:  843\nEpoch:  844\nEpoch:  845\nEpoch:  846\nEpoch:  847\nEpoch:  848\nEpoch:  849\nEpoch:  850\nEpoch:  851\nEpoch:  852\nEpoch:  853\nEpoch:  854\nEpoch:  855\nEpoch:  856\nEpoch:  857\nEpoch:  858\nEpoch:  859\nEpoch:  860\nEpoch:  861\nEpoch:  862\nEpoch:  863\nEpoch:  864\nEpoch:  865\nEpoch:  866\nEpoch:  867\nEpoch:  868\nEpoch:  869\nEpoch:  870\nEpoch:  871\nEpoch:  872\nEpoch:  873\nEpoch:  874\nEpoch:  875\nEpoch:  876\nEpoch:  877\nEpoch:  878\nEpoch:  879\nEpoch:  880\nEpoch:  881\nEpoch:  882\nEpoch:  883\nEpoch:  884\nEpoch:  885\nEpoch:  886\nEpoch:  887\nEpoch:  888\nEpoch:  889\nEpoch:  890\nEpoch:  891\nEpoch:  892\nEpoch:  893\nEpoch:  894\nEpoch:  895\nEpoch:  896\nEpoch:  897\nEpoch:  898\nEpoch:  899\nEpoch:  900\nEpoch:  901\nEpoch:  902\nEpoch:  903\nEpoch:  904\nEpoch:  905\nEpoch:  906\nEpoch:  907\nEpoch:  908\nEpoch:  909\nEpoch:  910\nEpoch:  911\nEpoch:  912\nEpoch:  913\nEpoch:  914\nEpoch:  915\nEpoch:  916\nEpoch:  917\nEpoch:  918\nEpoch:  919\nEpoch:  920\nEpoch:  921\nEpoch:  922\nEpoch:  923\nEpoch:  924\nEpoch:  925\nEpoch:  926\nEpoch:  927\nEpoch:  928\nEpoch:  929\nEpoch:  930\nEpoch:  931\nEpoch:  932\nEpoch:  933\nEpoch:  934\nEpoch:  935\nEpoch:  936\nEpoch:  937\nEpoch:  938\nEpoch:  939\nEpoch:  940\nEpoch:  941\nEpoch:  942\nEpoch:  943\nEpoch:  944\nEpoch:  945\nEpoch:  946\nEpoch:  947\nEpoch:  948\nEpoch:  949\nEpoch:  950\nEpoch:  951\nEpoch:  952\nEpoch:  953\nEpoch:  954\nEpoch:  955\nEpoch:  956\nEpoch:  957\nEpoch:  958\nEpoch:  959\nEpoch:  960\nEpoch:  961\nEpoch:  962\nEpoch:  963\nEpoch:  964\nEpoch:  965\nEpoch:  966\nEpoch:  967\nEpoch:  968\nEpoch:  969\nEpoch:  970\nEpoch:  971\nEpoch:  972\nEpoch:  973\nEpoch:  974\nEpoch:  975\nEpoch:  976\nEpoch:  977\nEpoch:  978\nEpoch:  979\nEpoch:  980\nEpoch:  981\nEpoch:  982\nEpoch:  983\nEpoch:  984\nEpoch:  985\nEpoch:  986\nEpoch:  987\nEpoch:  988\nEpoch:  989\nEpoch:  990\nEpoch:  991\nEpoch:  992\nEpoch:  993\nEpoch:  994\nEpoch:  995\nEpoch:  996\nEpoch:  997\nEpoch:  998\nEpoch:  999\nEpoch:  1000\n"
    }
   ],
   "source": [
    "train(RNN,data,batch=16,epochs=1000,interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'yo,allwa/r.f xd.lhb-sd bo..   r  lhohooghdvidf6.otf/2.to  br /6tlwl.qfktafdfurv\\n  d \\ndsvnd:sff3vhl .uh vs\\nh-nvabb\\novlj lhbjtorh\\nnm\\notf5s vlwv'"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "sentence(RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[100 111 110  97 108 100  32 116]\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_1_input to have 2 dimensions, but got array with shape (1, 8, 1)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-073fb70979b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'l'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0ms1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'd'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0ms2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'd'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2410\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    571\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    574\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected embedding_1_input to have 2 dimensions, but got array with shape (1, 8, 1)"
     ]
    }
   ],
   "source": [
    "RNN.reset_states()\n",
    "arr = np.array([ord('d'),ord('o'),ord('n'),ord('a'),ord('l'),ord('d'),ord(' '),ord('t')])\n",
    "print(arr)\n",
    "ps = RNN.predict(arr.reshape((1,8,1)))\n",
    "s1 = 'd'\n",
    "s2 = 'd'\n",
    "for p in ps[0]:\n",
    "  s1 = s1 + chr(np.random.choice(range(128),p=p.reshape(128)))\n",
    "  s2 = s2 +chr(np.argmax(p))\n",
    "print('donald t')\n",
    "print(s1)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitnnconda11252085119d4b49b0b65c12e80b6aba",
   "display_name": "Python 3.7.7 64-bit ('nn': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}